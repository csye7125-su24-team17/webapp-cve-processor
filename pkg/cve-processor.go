package pkg

import (
	"fmt"
	"log"
	"os"
	"path/filepath"

	"github.com/IBM/sarama"
)

// ProcessCVEFiles walks through all files in the specified directory and processes each JSON file.
func PushToKafka(rootPath string) {
	err := filepath.Walk(rootPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err 
		}
		if filepath.Ext(path) == ".json" { // Only process files with a .json extension
			processFile(path)
		}
		return nil
	})
	if err != nil {
		log.Printf("Error walking through CVE directory: %v", err)
	}
}

// processFile reads and processes a single JSON file from the given path.
func processFile(path string) {

	// Get the file name
	fileName := filepath.Base(path)

	// Skip delta.json and deltaLog.json files
	if fileName == "delta.json" || fileName == "deltaLog.json" {
		log.Printf("Skipping file: %s", fileName)
		return
	}
	data, err := os.ReadFile(path) // Read the file content
	if err != nil {
		log.Printf("Error reading file %s: %v", path, err)
		return
	}
	kafkaTopic := os.Getenv("kafka_topic")
	pushToTopic(kafkaTopic, data)
}

func ConnectProducer(brokersUrl []string)(sarama.SyncProducer, error){
	config := sarama.NewConfig()
	config.Producer.Return.Successes = true
	config.Producer.RequiredAcks = sarama.WaitForAll
	config.Producer.Retry.Max = 5
	conn,err:= sarama.NewSyncProducer(brokersUrl,config)
	if err!=nil{
		return nil, err
	}
	return conn,nil
}

func pushToTopic(topic string, jsonData []byte) error {
	kafkaHost := os.Getenv("kafka_host")
	kafkaPort := os.Getenv("kafka_port")
	kafkaUrl := fmt.Sprintf("%s:%s",kafkaHost,kafkaPort)
	brokersUrl := []string{kafkaUrl}
	producer, err := ConnectProducer(brokersUrl)
	if err!=nil{
		return err
	}
	defer producer.Close()
	msg := &sarama.ProducerMessage{
		Topic: topic,
		Value: sarama.StringEncoder(jsonData),
	}
	partition, offset, err := producer.SendMessage(msg)
	if err != nil{
		return err
	}
	fmt.Printf("Message is stored in topic(%s)/partition(%d)/offset(%d)\n",topic,partition,offset)
	return nil
}